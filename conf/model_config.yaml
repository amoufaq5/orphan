# =========================
# Orph — Base Model Config
# ~300M params, GPT-style decoder-only
# =========================
model_type: decoder_only

# Token embedding / vocab
vocab_size: 32000          # ← must match your tokenizer
pad_token_id: 0
eos_token_id: 1
bos_token_id: 1
tie_word_embeddings: true

# Transformer backbone
hidden_size: 1024
intermediate_size: 2816     # ~2.75x hidden is common (can be 4x too: 4096)
num_hidden_layers: 18
num_attention_heads: 16
num_key_value_heads: 16     # MHA; set 8 if you do MQA/GQA
max_position_embeddings: 2048
rotary_emb_base: 10000
rope_scaling: null          # set if you need beyond 2k ctx
attn_impl: flash            # if your code supports FA2 on CUDA 12; else "standard"

# Regularization
dropout: 0.0
attention_dropout: 0.0
layer_norm_eps: 1e-5

# Init / scaling
initializer_range: 0.02
resid_pdrop: 0.0

# KV cache / inference
use_cache: true

# Precision/runtime hints (trainer may read these)
precision_hint: bf16
gradient_checkpointing: true
torch_compile_default: true
