_arch: gpt_neox
hidden_size: 1536
num_hidden_layers: 24
num_attention_heads: 12
intermediate_size: 6144
max_position_embeddings: 2048
rotary_pct: 1.0
rotary_emb_base: 10000
layer_norm_eps: 1.0e-5
initializer_range: 0.02
bos_token_id: 1
eos_token_id: 2
pad_token_id: 0
use_cache: false
