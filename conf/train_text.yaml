# conf/train_text.yaml
# Config for base TextLM training / continue-pretraining

# Path to unified training corpus (built via make_corpus.py)
train_path: "data/corpus/corpus.jsonl.gz"

# Output directory for checkpoints
out_dir: "out/models/textlm"

# Tokenizer to use (must exist or be trained already)
tokenizer_path: "out/tokenizer"

# Training parameters
epochs: 3
train_batch_size: 16
grad_accum: 2
max_length: 512
lr: 5e-5
weight_decay: 0.01
warmup_ratio: 0.05

# Evaluation split (0.01 = 1%)
eval_split: 0.01

# Logging / saving
logging_steps: 100
save_steps: 500

# Random seed for reproducibility
seed: 42
