# conf/train_text.yaml
# Training config for your from-scratch GPT-style LM

# ---- Data & Paths ----
train_path: "data/corpus/corpus.jsonl.gz"   # unified corpus
out_dir: "out/models/textlm"                # checkpoints + final model
tokenizer_path: "out/tokenizer"             # your ByteLevel BPE tokenizer
model_config: "conf/model_config.yaml"      # architecture (~140M)

# ---- Training ----
epochs: 3
train_batch_size: 8         # per-device batch size
grad_accum: 1               # gradient accumulation steps
max_length: 512             # sequence length for tokenization

# ---- Optimizer / Schedule ----
lr: 0.00005                 # 5e-5 (float, not quoted)
weight_decay: 0.01
warmup_ratio: 0.05

# ---- Eval / Logging / Checkpointing ----
eval_split: 0.01            # 1% of data for eval
logging_steps: 100
save_steps: 500             # save checkpoint every N steps
seed: 42

# ---- (Optional) Debug downsampling ----
# max_train_samples: 50000
# max_eval_samples: 2000
