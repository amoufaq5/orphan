# conf/train_text.yaml
# Training config for your from-scratch GPT-style LM

# ---- Data & Paths ----
train_path: "data/corpus/corpus.jsonl.gz"   # unified corpus
out_dir: "out/models/textlm"                # checkpoints + final model
tokenizer_path: "out/tokenizer"             # your ByteLevel BPE tokenizer
model_config: "conf/model_config.yaml"      # architecture (~140M)

# ---- Training ----
epochs: 3
train_batch_size: 8         # per-device batch size
grad_accum: 1               # gradient accumulation steps
max_length: 512             # sequence length for tokenization

# ---- Optimizer / Schedule ----
lr: 0.00005                 # 5e-5 (float, not quoted)
weight_decay: 0.01
warmup_ratio: 0.05

# ---- Eval / Logging / Checkpointing ----
eval_split: 0.01            # 1% of data for eval
logging_steps: 100
save_steps: 500             # save checkpoint every N steps
seed: 42

# Performance (GPU)
fp16: true        # set bf16: true & fp16: false if your stack/GPU supports bf16 well
bf16: false
num_workers: 4    # bump to 8 if dataloader is the bottleneck
pin_memory: auto  # our code treats 'auto' as True on GPU, False on CPU
grad_ckpt: true   # reduces VRAM; a bit slower but safer

# Throughput
train_batch_size: 16   # if OOM, try 8 or 4, and raise grad_accum accordingly
grad_accum: 2          # effective batch = batch_size * grad_accum * num_gpus
max_length: 512        # if OOM, try 384 or 256

# Logging/ckpt (keep)
logging_steps: 100
save_steps: 1000       # fewer, larger checkpoints for long runs
