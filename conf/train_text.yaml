train:
  tokenizer:
    vocab_size: 52000
    min_freq: 2
    model_type: spm_unigram
    character_coverage: 0.9995
    byte_fallback: true
    lowercase: false
    normalization: "NFKC"
    special_tokens: ["<pad>", "<s>", "</s>", "<unk>", "<triage>", "<referral>", "<cite>", "<arabic>", "<english>"]

  textlm:
    model_dim: 768
    n_layers: 16
    n_heads: 12
    ffn_mult: 4
    norm: "rms"         # "rms" | "layernorm"
    rope: true
    context_len: 8192
    dropout: 0.1

  optimization:
    batch_size: 16
    grad_accum: 2
    lr: 3.0e-4
    weight_decay: 0.05
    warmup_ratio: 0.06
    epochs: 3
    betas: [0.9, 0.95]
    eps: 1.0e-8
    max_grad_norm: 1.0
    amp: true
    seed: 1337

  data:
    train_glob: "data/canonical/canonical*.jsonl"
    eval_ratio: 0.01
    min_seq_len: 16
    max_seq_len: 8192
    text_fields: ["sections.body"]   # dot-paths inside CanonicalDoc rows
    shuffle_buffer: 20000            # lines buffered before shuffle
    pack_sequences: true             # efficient packing into full context windows

  io:
    out_dir: "out/text_orphgpt"
    save_every_steps: 1000
    eval_every_steps: 1000
    log_every_steps: 100
    keep_last_n: 3

  tokenizer_files:
    spm_model: "out/tokenizer/orph_spm.model"   # produced by train_tokenizer_spm.py
    special_tokens: ["<pad>", "<s>", "</s>", "<unk>", "<triage>", "<referral>", "<cite>", "<arabic>", "<english>"]
