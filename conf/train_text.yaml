# Data
train_jsonl_glob: "data/cleaned/**/*.jsonl" # or data/raw/**/part-*.jsonl after a packing step
text_key: "text" # adjust to your schema
block_size: 512 # tokens per example

# Output
output_dir: "out/text_orphgpt"
logging_steps: 20
save_steps: 2000
eval_steps: 2000
save_total_limit: 3

# Train
batch_size_per_device: 1
gradient_accumulation_steps: 16
num_train_epochs: 1
learning_rate: 3.0e-4
warmup_ratio: 0.01
weight_decay: 0.1

# Precision & mem
bf16: true
gradient_checkpointing: true
flash_attention: true

# Tokenizer
tokenizer_dir: "out/tokenizer" # or a pretrained tokenizer path
